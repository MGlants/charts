image:
  repository: goofball222/pritunl
  pullPolicy: Always

# This should match whatever the 'mongodb' service is called in the cluster.
# DNS should be able to resolve the service by this name for Pritunl to function.
pritunl:
  mongoURI: "mongodb://pritunl:pritunl@mongodb:27017/pritunl"
  wireguard: "false"
  debug: "false"
  opts: ""

# Change these as you see fit.
ports:
  http: 80
  https: 443
  openvpn: 1194
  wireguard: 1195
ingress:
  enabled: true
  ingressClassName: nginx
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-production"
  hosts: [pritunl.oke.byvko.dev]
  tls:
  - hosts:
    - pritunl.oke.byvko.dev
    secretName: pritunl-tls-cert

# This must be enabled when using Pritunl due to the rights that iptables will need in the Pritunl pods.
privileged:
  enabled: true

# This will adjust the replicas that are deployed as a part of the Pritunl deployment.
# This is '1' by default. Your Pritunl cluster number will be affected by this.
replicas: 1

# If 'type' here is 'LoadBalancer', these annotations are necessary to properly use an ELB if deploying this chart in AWS, so they'll automatically get used.
# Be sure to add the appropriate domain name, cert ARN, and ssl-negotiation-policy (a default is used here).
service:
  annotations: {}
  externalTrafficPolicy: Local
  
  type: LoadBalancer

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
nodeSelector: {}

tolerations: []

affinity: {}